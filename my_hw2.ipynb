{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled43.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOlvbo6xnhfx8vQqijO3i5p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryKutsev/DeepHW/blob/master/my_hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkCUQ7QX3CF5",
        "outputId": "02018c98-389f-4af0-b60d-14c474f475e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-12 17:30:53--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  28.9MB/s    in 23s     \n",
            "\n",
            "2020-10-12 17:31:17 (27.7 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38TD-0q43J7G",
        "outputId": "7e54c9a9-5668-4983-8646-2f334920eee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/BobaZooba/HSE-Deep-Learning-in-NLP-Course/master/Week%203/data.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-12 17:31:17--  https://raw.githubusercontent.com/BobaZooba/HSE-Deep-Learning-in-NLP-Course/master/Week%203/data.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10563 (10K) [text/plain]\n",
            "Saving to: ‘data.py’\n",
            "\n",
            "data.py             100%[===================>]  10.32K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-10-12 17:31:18 (13.2 MB/s) - ‘data.py’ saved [10563/10563]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ4O-_ZN3QZo"
      },
      "source": [
        "!mv data.py mydata.py"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jr1Wb2V3L9u"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import zipfile\n",
        "from torch import nn\n",
        "import seaborn as sns\n",
        "from mydata import Downloader, Parser"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01Lso6k7NV3Z"
      },
      "source": [
        "### **DATA**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwtFMLs23WBV"
      },
      "source": [
        "data_path = './data/'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MODAu4I73Wbn"
      },
      "source": [
        "downloader = Downloader(data_path=data_path)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMpwL6T-3ZYR",
        "outputId": "49f16557-c8a5-47fc-8c48-49e809eb3196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "downloader.run()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "single: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s]\n",
            "multiple: 100%|██████████| 17/17 [00:16<00:00,  1.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlY-Ks5C3cE1"
      },
      "source": [
        "parser = Parser(data_path=data_path)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghoO5tyo3gSg",
        "outputId": "a9b16977-a35e-4abf-9195-179e18bd8ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "unlabeled, train, valid = parser.run()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading: 100%|██████████| 38/38 [02:36<00:00,  4.13s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0HM1qwMGLsg",
        "outputId": "d704a9ea-1245-4d67-c2f5-9f380d90f4bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "s = 'ривет привет как дела? ха, дерьмо.'\n",
        "wordpunct_tokenize(s)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ривет', 'привет', 'как', 'дела', '?', 'ха', ',', 'дерьмо', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGWYk_aZOmTN"
      },
      "source": [
        "unique_categories = set(train.category.unique().tolist() + valid.category.unique().tolist())\n",
        "\n",
        "category2index = {category: index for index, category in enumerate(unique_categories)}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEf1B96XOcYN"
      },
      "source": [
        "train['target'] = train.category.map(category2index)\n",
        "valid['target'] = valid.category.map(category2index)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVmXIIhiOlmw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhxrwsA83ilr"
      },
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, texts, targets, vocab, pad_index=0, max_length=32):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.texts = texts\n",
        "        self.targets = targets\n",
        "        self.vocab = vocab\n",
        "        \n",
        "        self.pad_index = pad_index\n",
        "        self.max_length = max_length\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def tokenization(self, text):\n",
        "        \n",
        "        tokens = wordpunct_tokenize(text)\n",
        "        \n",
        "        token_indices = [self.vocab[tok] for tok in tokens if tok in self.vocab]\n",
        "        \n",
        "        return token_indices\n",
        "    \n",
        "    def padding(self, tokenized_text):\n",
        "        \n",
        "        tokenized_text = tokenized_text[:self.max_length]\n",
        "        \n",
        "        tokenized_text += [self.pad_index] * (self.max_length - len(tokenized_text))\n",
        "        \n",
        "        return tokenized_text\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        text = self.texts[index]        \n",
        "        target = self.targets[index]\n",
        "        \n",
        "        tokenized_text = self.tokenization(text)\n",
        "        tokenized_text = self.padding(tokenized_text)\n",
        "        \n",
        "        tokenized_text = torch.tensor(tokenized_text)\n",
        "        \n",
        "        return tokenized_text, target"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGbmX9-UNZuB"
      },
      "source": [
        "train_x = list(train.question)\n",
        "train_y = list(train.target)\n",
        "\n",
        "valid_x = list(valid.question)\n",
        "valid_y = list(valid.target)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blf5KOf9NZy5"
      },
      "source": [
        "train_dataset = TextClassificationDataset(texts=train_x, targets=train_y, vocab=vocab)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oqA8o9-NZw7"
      },
      "source": [
        "x, y = train_dataset[0]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R-El2RNLDDC"
      },
      "source": [
        "train_dataset = TextClassificationDataset(texts=train_x, targets=train_y, vocab=vocab)\n",
        "valid_dataset = TextClassificationDataset(texts=valid_x, targets=valid_y, vocab=vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=128)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mFhzvvtPI7q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnw0RV3jPI-l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zbcfOHjPJBy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP49pfe2PbWY"
      },
      "source": [
        "### **Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mno04x_ALDGE"
      },
      "source": [
        "def load_embeddings(zip_path, filename, pad_token='PAD', max_words=100_000, verbose=True):\n",
        "    \n",
        "    vocab = dict()\n",
        "    embeddings = list()\n",
        "\n",
        "    with zipfile.ZipFile(zip_path) as zipped_file:\n",
        "        with zipped_file.open(filename) as file_object:\n",
        "\n",
        "            vocab_size, embedding_dim = file_object.readline().decode('utf-8').strip().split()\n",
        "\n",
        "            vocab_size = int(vocab_size)\n",
        "            embedding_dim = int(embedding_dim)\n",
        "            \n",
        "            # в файле 1 000 000 слов с векторами, давайте ограничим для простоты этот словарь\n",
        "            max_words = vocab_size if max_words <= 0 else max_words\n",
        "            \n",
        "            # добавим пад токен и эмбеддинг в нашу матрицу эмбеддингов и словарь\n",
        "            vocab[pad_token] = len(vocab)\n",
        "            embeddings.append(np.zeros(embedding_dim))\n",
        "\n",
        "            progress_bar = tqdm(total=max_words, disable=not verbose)\n",
        "\n",
        "            for line in file_object:\n",
        "                parts = line.decode('utf-8').strip().split()\n",
        "\n",
        "                token = ' '.join(parts[:-embedding_dim]).lower()\n",
        "                \n",
        "                if token in vocab:\n",
        "                    continue\n",
        "                \n",
        "                word_vector = np.array(list(map(float, parts[-embedding_dim:])))\n",
        "\n",
        "                vocab[token] = len(vocab)\n",
        "                embeddings.append(word_vector)\n",
        "\n",
        "                progress_bar.update()\n",
        "                \n",
        "                if len(vocab) == max_words:\n",
        "                    break\n",
        "\n",
        "            progress_bar.close()\n",
        "\n",
        "    embeddings = np.stack(embeddings)\n",
        "    \n",
        "    return vocab, embeddings"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMmmebKSLDIz",
        "outputId": "6b4c23ba-352d-4186-c63c-e84fab5326a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab, embeddings = load_embeddings('./wiki-news-300d-1M.vec.zip', 'wiki-news-300d-1M.vec', max_words=100_000)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 99999/100000 [00:12<00:00, 8020.66it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-eAlPDLLfhv"
      },
      "source": [
        "index2token = {index: token for token, index in vocab.items()}"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTWCVaU7Lfko"
      },
      "source": [
        "emb_norms = np.linalg.norm(embeddings, axis=1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPPEdPKuLfvm"
      },
      "source": [
        "embeddings = torch.tensor(embeddings).float()\n",
        "embedding_layer = nn.Embedding.from_pretrained(embeddings, padding_idx=0)\n",
        "x_embed = embedding_layer(x)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zIuu33dQgjq",
        "outputId": "ff54d711-9673-43ce-ae4c-72766f0b4997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(train_loader))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3vHSMvFQgo4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hD7D0jhQgml"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLmg8VjKLfyh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rREyIpOlQhQP"
      },
      "source": [
        "### **MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKg6xfn_GkLg"
      },
      "source": [
        "class DeepAverageNetwork(nn.Module):\n",
        "    \n",
        "    def __init__(self, embeddings, linear_1_size, linear_2_size, n_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embeddings, padding_idx=0)\n",
        "        \n",
        "        self.batch_norm = nn.BatchNorm1d(num_features=embeddings.shape[-1])\n",
        "        \n",
        "        self.linear_1 = nn.Linear(in_features=embeddings.shape[-1], out_features=linear_1_size)\n",
        "        self.linear_2 = nn.Linear(in_features=linear_1_size, out_features=linear_2_size)\n",
        "        self.linear_3 = nn.Linear(in_features=linear_2_size, out_features=n_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # переводим индексы слов в эмбеддинги этих слов\n",
        "        # (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
        "        x = self.embedding_layer(x)\n",
        "        \n",
        "        # агрегируем наши эмбеддинги по размерности время\n",
        "        # (batch_size, sequence_length, embedding_dim) -> (batch_size, embedding_dim)\n",
        "        x = x.sum(dim=1)\n",
        "        \n",
        "        # делаем нормирование\n",
        "        # (batch_size, embedding_dim) -> (batch_size, embedding_dim)\n",
        "        x = self.batch_norm(x)\n",
        "        \n",
        "        # прогоняем через первый линейный слой\n",
        "        # (batch_size, embedding_dim) -> (batch_size, linear_1_size)\n",
        "        x = self.linear_1(x)\n",
        "        \n",
        "        # применяем нелинейность\n",
        "        # (batch_size, linear_1_size) -> (batch_size, linear_1_size)\n",
        "        x = torch.relu(x)\n",
        "        \n",
        "        # прогоняем через второй линейный слой\n",
        "        # (batch_size, linear_1_size) -> (batch_size, linear_2_size)\n",
        "        x = self.linear_2(x)\n",
        "        \n",
        "        # применяем нелинейность\n",
        "        # (batch_size, linear_2_size) -> (batch_size, linear_2_size)\n",
        "        x = torch.relu(x)\n",
        "        \n",
        "        # переводим с помощью линейного преобразования в количество классов\n",
        "        # (batch_size, linear_2_size) -> (batch_size, n_classes)\n",
        "        x = self.linear_3(x)\n",
        "        \n",
        "        ## по идеи здесь должен был быть софтмакс\n",
        "        ## но мы будем использовать лосс nn.CrossEntropyLoss()\n",
        "        ## в его документации написано\n",
        "        ## This criterion combines :func:`nn.LogSoftmax` and :func:`nn.NLLLoss` in one single class.\n",
        "        ## это некоторая оптимизация, которая включает в себя сразу и софтмакс и сам negative log likelihood лосс\n",
        "        ## так как у нас в лоссе есть софтмакс, то мы не будем применять его в сетке\n",
        "        ## на этапе предсказания (а не обучения) мы будем отдельно делать софтмакс для получения распределения классов\n",
        "        ## \n",
        "        ## (batch_size, n_classes) -> (batch_size, n_classes)\n",
        "        # x = torch.softmax(x, dim=-1)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf8Lbe_AHOML"
      },
      "source": [
        "\n",
        "model = DeepAverageNetwork(embeddings=embeddings,\n",
        "                           linear_1_size=256, \n",
        "                           linear_2_size=128, \n",
        "                           n_classes=len(category2index))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVBeT2CpHTZg"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiOeGl71YxH6",
        "outputId": "1f1784c6-93b4-41d7-96dc-53864cfd4233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "epochs = 3\n",
        "losses = []\n",
        "for i in range(epochs):\n",
        "  for x, y in train_loader:\n",
        "    y_pred = model.forward(x)\n",
        "    loss = criterion(y_pred, y)\n",
        "    losses.append(loss) \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(f\"epoch {i}, loss {loss}\")\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, loss 0.9458043575286865\n",
            "epoch 1, loss 0.8962230682373047\n",
            "epoch 2, loss 0.8393556475639343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vOB22n9cE_n"
      },
      "source": [
        "losses = list()\n",
        "\n",
        "# это переводит модель в режим предсказания\n",
        "# то есть фиксируются статистики батч норма, дропаут не выкидывает фичи\n",
        "# model.eval()\n",
        "\n",
        "# заметьте, что мы поменяли наш лоадер на валидационный\n",
        "for x, y in valid_loader:\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # получение предсказаний модели\n",
        "        # расчет лосса\n",
        "        ...\n",
        "    \n",
        "    ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiGi1RrldsHu"
      },
      "source": [
        "losses = list()\n",
        "\n",
        "# это переводит модель в режим предсказания\n",
        "# то есть фиксируются статистики батч норма, дропаут не выкидывает фичи\n",
        "model.eval()\n",
        "\n",
        "# заметьте, что мы поменяли наш лоадер на валидационный\n",
        "for x, y in valid_loader:\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # получение предсказаний модели\n",
        "        # расчет лосса\n",
        "          y_pred = model.forward(x)\n",
        "          loss = criterion(y_pred, y)\n",
        "          losses.append(loss)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}