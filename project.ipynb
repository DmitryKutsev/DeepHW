{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled46.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOwaBuRTNtt/FZjYxTLJdDa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryKutsev/DeepHW/blob/master/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QQohLwxFEf0",
        "outputId": "33815c6d-0a97-4224-cd41-a20a508b5ccf"
      },
      "source": [
        "!pip install tinysegmenter"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tinysegmenter\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/70/488895cb11e160b548c9ba5847c171b65b86a8ca1e54d206d55b2976bf7b/tinysegmenter-0.4.tar.gz\n",
            "Building wheels for collected packages: tinysegmenter\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.4-cp36-none-any.whl size=13536 sha256=e88266293c8aaad6ef234a425ac8fa1b42f944b14152b056bb658a3546d1a026\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/71/2b/6402196bf28012826e507ef7b99df6ebd98cce78bd99023471\n",
            "Successfully built tinysegmenter\n",
            "Installing collected packages: tinysegmenter\n",
            "Successfully installed tinysegmenter-0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffN6mQimAG0O"
      },
      "source": [
        "*Kurohashi-Kawahara Lab. has the copyright of Japanese Basic Sentence Data, and NICT MASTAR Project, Multilingual Translation Lab. has the copyright of English and Chinese Basic Sentence Data. You can use all the data under the terms of the Creative Commons Attribution 3.0 Unported license.\n",
        "     http://nlp.ist.i.kyoto-u.ac.jp/EN/?JEC%20Basic%20Sentence%20Data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z76bIavR4OgJ"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import tinysegmenter"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EykjZGFFOgV"
      },
      "source": [
        "segmenter = tinysegmenter.TinySegmenter()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbLZV20E6xww"
      },
      "source": [
        "device = torch.device('cuda:0')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhuE25fp8kyo"
      },
      "source": [
        "my_frame = pd.read_excel('http://nlp.ist.i.kyoto-u.ac.jp/EN/?plugin=attach&refer=JEC%20Basic%20Sentence%20Data&openfile=JEC_basic_sentence_v1-2.xls')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF5b8Q-1-NCM"
      },
      "source": [
        "#remove Chineese column\n",
        "my_frame = my_frame.drop(['难道不会是X吗，我实在是感到怀疑。'], axis=1)\n",
        "my_frame.columns = ['index', 'jap', 'eng']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "or_8lZYz-TSy",
        "outputId": "8f909663-585c-4bdd-bdb1-f45d7cf89499"
      },
      "source": [
        "my_frame"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>jap</th>\n",
              "      <th>eng</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#0002</td>\n",
              "      <td>Xがいいなといつも思います</td>\n",
              "      <td>I always think X would be nice.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#0003</td>\n",
              "      <td>それがあるようにいつも思います</td>\n",
              "      <td>It always seems like it is there.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#0004</td>\n",
              "      <td>それが多すぎないかと正直思う</td>\n",
              "      <td>I honestly feel like there is too much.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#0005</td>\n",
              "      <td>山田はみんなに好かれるタイプの人だと思う</td>\n",
              "      <td>I think that Yamada is the type everybody likes.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#0006</td>\n",
              "      <td>〜と誰かが思った</td>\n",
              "      <td>Someone thought that 〜</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5298</th>\n",
              "      <td>#5300</td>\n",
              "      <td>チームが４人のメンバーで構成されています</td>\n",
              "      <td>The team consists of four members.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5299</th>\n",
              "      <td>#5301</td>\n",
              "      <td>彼が実際に動画を再生する</td>\n",
              "      <td>He actually plays the video.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5300</th>\n",
              "      <td>#5302</td>\n",
              "      <td>政府が銀行に公的資金をどんどん投入しました</td>\n",
              "      <td>The government injected massive public funds i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5301</th>\n",
              "      <td>#5303</td>\n",
              "      <td>レベル１の機能に下記の機能をプラスする</td>\n",
              "      <td>The following will be added to the level 1 fun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5302</th>\n",
              "      <td>#5304</td>\n",
              "      <td>彼が携帯を制服のポケットに仕舞う</td>\n",
              "      <td>He puts his cell phone into the pocket of his ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5303 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  ...                                                eng\n",
              "0     #0002  ...                    I always think X would be nice.\n",
              "1     #0003  ...                  It always seems like it is there.\n",
              "2     #0004  ...            I honestly feel like there is too much.\n",
              "3     #0005  ...   I think that Yamada is the type everybody likes.\n",
              "4     #0006  ...                             Someone thought that 〜\n",
              "...     ...  ...                                                ...\n",
              "5298  #5300  ...                 The team consists of four members.\n",
              "5299  #5301  ...                       He actually plays the video.\n",
              "5300  #5302  ...  The government injected massive public funds i...\n",
              "5301  #5303  ...  The following will be added to the level 1 fun...\n",
              "5302  #5304  ...  He puts his cell phone into the pocket of his ...\n",
              "\n",
              "[5303 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NR5I8JtFW30",
        "outputId": "e83a2e35-1fdd-43d1-b47a-09a65f7e87cc"
      },
      "source": [
        "segmenter.tokenize(my_frame['jap'][0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['X', 'が', 'いい', 'な', 'といつも', '思い', 'ます']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlOIJb8gCZke"
      },
      "source": [
        "# pairs = [list()]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3C8Z7as_8aY"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYfkUoKCA_Qx"
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    # s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i03rUdX1kzx5",
        "outputId": "d29e9d3c-f062-4273-d057-400aceb1d290"
      },
      "source": [
        "ppp = [[11],[33]]\n",
        "ppp = list(reversed(ppp))\n",
        "ppp"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[33], [11]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87Lc2S6govZK",
        "outputId": "32f5850c-9e96-4fab-baae-d0a395648702"
      },
      "source": [
        "    for index, sent in enumerate(my_frame['jap']):\n",
        "      if index == 1:\n",
        "        print(index, sent)\n",
        "      pair = [normalizeString(my_frame['eng'][index]), ' '.join(segmenter.tokenize(sent))]\n",
        "      if index == 1:\n",
        "        print(pair)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 それがあるようにいつも思います\n",
            "['It always seems like it is there .', 'それ が ある よう にいつも 思い ます']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T22fh0XBFLN"
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    # lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "    #     read().strip().split('\\n')\n",
        "    pairs = []\n",
        "    # Split every line into pairs and normalize\n",
        "    for index, sent in enumerate(my_frame['jap']):\n",
        "      pair = [normalizeString(my_frame['eng'][index]), ' '.join(segmenter.tokenize(sent))]\n",
        "      pairs.append(pair)\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoVdlZE5BJkD"
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "        # p[0].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEJ35nWlBOVg",
        "outputId": "4d2507c9-12b2-4e54-c302-506ddb56e1bd"
      },
      "source": [
        "\n",
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'jap', False)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 5303 sentence pairs\n",
            "Trimmed to 5209 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 5753\n",
            "jap 6963\n",
            "['I did a live show with him at a club .', '彼 と 一緒 に ライブハウス で ライブ を やっ た']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH-wNL4-BR_q"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Md8F4vSu-B8"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlZ-64HJvLD_"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyRvQh6VvPnE"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prCzxljCvai-"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBhWbXTlqXFv",
        "outputId": "ebb05000-c29e-43f9-958c-8dc7bca86738",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.AttnDecoderRNN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4mx4TiOvfeS"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-LADdCNvj-4"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXJkIiK0voTu"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyE4P_7Svq5x"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ky1FK7KvuiM"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnuDEsCapRLv"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMbJEQ_Ypl3O",
        "outputId": "2da38f36-b427-49ca-dd98-7396f3ddac1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_lang.n_words"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5753"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m0L7hpKpulC",
        "outputId": "9bd91f8f-31f2-4c66-9ec0-04151feb7347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluate(encoder1, attn_decoder1, \"I want to kill\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['流れ',\n",
              "  '構成',\n",
              "  '横道',\n",
              "  '遭う',\n",
              "  '勧め',\n",
              "  '世紀',\n",
              "  '大正',\n",
              "  '短期',\n",
              "  '摂り',\n",
              "  '向こう',\n",
              "  '全',\n",
              "  '世紀',\n",
              "  '大正',\n",
              "  '短期',\n",
              "  '摂り',\n",
              "  '向こう',\n",
              "  '全',\n",
              "  '世紀',\n",
              "  '世紀',\n",
              "  '使わ'],\n",
              " tensor([[0.1022, 0.0646, 0.0436, 0.0425, 0.0347, 0.0374, 0.0311, 0.0488, 0.0547,\n",
              "          0.0463, 0.0558, 0.0339, 0.0490, 0.0560, 0.0326, 0.0555, 0.0666, 0.0475,\n",
              "          0.0551, 0.0421],\n",
              "         [0.0174, 0.0647, 0.0576, 0.0383, 0.0302, 0.0422, 0.0413, 0.0252, 0.1138,\n",
              "          0.0530, 0.0828, 0.0410, 0.0429, 0.0632, 0.0361, 0.0413, 0.0281, 0.0354,\n",
              "          0.0611, 0.0844],\n",
              "         [0.0317, 0.0618, 0.0390, 0.0262, 0.0515, 0.0669, 0.0440, 0.0762, 0.0235,\n",
              "          0.0530, 0.0401, 0.0428, 0.0583, 0.0519, 0.0357, 0.0548, 0.0445, 0.0312,\n",
              "          0.0780, 0.0889],\n",
              "         [0.0599, 0.0982, 0.0802, 0.0350, 0.0433, 0.0324, 0.0607, 0.0485, 0.0293,\n",
              "          0.0237, 0.0575, 0.0715, 0.0200, 0.0318, 0.0787, 0.0313, 0.0679, 0.0203,\n",
              "          0.0181, 0.0917],\n",
              "         [0.0887, 0.0488, 0.0708, 0.0493, 0.0693, 0.0353, 0.0497, 0.0184, 0.0316,\n",
              "          0.0477, 0.0606, 0.0384, 0.0496, 0.0435, 0.0700, 0.0239, 0.0543, 0.0330,\n",
              "          0.0483, 0.0687],\n",
              "         [0.0846, 0.0256, 0.0444, 0.0421, 0.0228, 0.0672, 0.0553, 0.0570, 0.0224,\n",
              "          0.0231, 0.0988, 0.0602, 0.0678, 0.0485, 0.0731, 0.0713, 0.0264, 0.0422,\n",
              "          0.0341, 0.0334],\n",
              "         [0.0249, 0.0326, 0.0319, 0.0361, 0.0472, 0.0531, 0.1001, 0.0628, 0.0643,\n",
              "          0.0490, 0.0374, 0.0395, 0.0708, 0.0517, 0.0242, 0.0386, 0.0491, 0.0922,\n",
              "          0.0456, 0.0489],\n",
              "         [0.0857, 0.1177, 0.0341, 0.0474, 0.0259, 0.0244, 0.0386, 0.0375, 0.0739,\n",
              "          0.0294, 0.0650, 0.0207, 0.0636, 0.0336, 0.0547, 0.0384, 0.0736, 0.0260,\n",
              "          0.0206, 0.0892],\n",
              "         [0.0235, 0.0923, 0.0585, 0.0166, 0.0407, 0.0372, 0.1038, 0.0174, 0.0595,\n",
              "          0.0560, 0.0225, 0.0856, 0.1227, 0.0443, 0.0347, 0.0300, 0.0634, 0.0274,\n",
              "          0.0182, 0.0456],\n",
              "         [0.0602, 0.0561, 0.0672, 0.0525, 0.0493, 0.0454, 0.0550, 0.0547, 0.0575,\n",
              "          0.0243, 0.0319, 0.0671, 0.0225, 0.0379, 0.0354, 0.0312, 0.0411, 0.1177,\n",
              "          0.0599, 0.0330],\n",
              "         [0.0357, 0.0413, 0.0371, 0.0275, 0.0412, 0.0529, 0.0373, 0.0599, 0.0785,\n",
              "          0.0398, 0.0537, 0.0639, 0.0609, 0.0348, 0.0966, 0.0526, 0.0400, 0.0460,\n",
              "          0.0360, 0.0644],\n",
              "         [0.0354, 0.0530, 0.0383, 0.0532, 0.0319, 0.0380, 0.0350, 0.0246, 0.1017,\n",
              "          0.0826, 0.0584, 0.0388, 0.0393, 0.0487, 0.0924, 0.0250, 0.0261, 0.0614,\n",
              "          0.0488, 0.0671],\n",
              "         [0.0227, 0.0313, 0.0356, 0.0506, 0.0431, 0.0377, 0.0688, 0.0612, 0.0658,\n",
              "          0.0522, 0.0351, 0.0463, 0.0731, 0.0518, 0.0382, 0.0454, 0.0534, 0.1078,\n",
              "          0.0428, 0.0371],\n",
              "         [0.0708, 0.1064, 0.0362, 0.0430, 0.0281, 0.0235, 0.0298, 0.0372, 0.0728,\n",
              "          0.0313, 0.0681, 0.0203, 0.0602, 0.0349, 0.0590, 0.0464, 0.0596, 0.0326,\n",
              "          0.0329, 0.1068],\n",
              "         [0.0287, 0.0731, 0.0548, 0.0214, 0.0470, 0.0325, 0.0975, 0.0193, 0.0662,\n",
              "          0.0627, 0.0255, 0.0814, 0.1205, 0.0431, 0.0379, 0.0323, 0.0501, 0.0356,\n",
              "          0.0227, 0.0478],\n",
              "         [0.0690, 0.0567, 0.0742, 0.0549, 0.0558, 0.0600, 0.0391, 0.0573, 0.0494,\n",
              "          0.0273, 0.0356, 0.0490, 0.0259, 0.0297, 0.0339, 0.0343, 0.0366, 0.1011,\n",
              "          0.0617, 0.0486],\n",
              "         [0.0459, 0.0465, 0.0290, 0.0353, 0.0341, 0.0483, 0.0581, 0.0655, 0.0825,\n",
              "          0.0394, 0.0471, 0.0689, 0.0567, 0.0293, 0.0869, 0.0595, 0.0419, 0.0427,\n",
              "          0.0296, 0.0525],\n",
              "         [0.0271, 0.0591, 0.0387, 0.0463, 0.0434, 0.0374, 0.0477, 0.0249, 0.1044,\n",
              "          0.1155, 0.0526, 0.0424, 0.0434, 0.0446, 0.0673, 0.0216, 0.0191, 0.0574,\n",
              "          0.0420, 0.0654],\n",
              "         [0.0233, 0.0335, 0.0289, 0.0461, 0.0536, 0.0493, 0.0841, 0.0601, 0.0623,\n",
              "          0.0555, 0.0357, 0.0453, 0.0641, 0.0429, 0.0251, 0.0393, 0.0532, 0.1202,\n",
              "          0.0449, 0.0328],\n",
              "         [0.0318, 0.0371, 0.0391, 0.0469, 0.0553, 0.0600, 0.0820, 0.0493, 0.0476,\n",
              "          0.0593, 0.0367, 0.0532, 0.0539, 0.0498, 0.0338, 0.0310, 0.0555, 0.1056,\n",
              "          0.0394, 0.0326]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOkBreaXvyQO",
        "outputId": "4d885179-b8ab-400d-e28b-aff938df0902"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2m 19s (- 32m 27s) (5000 6%) 4.6875\n",
            "4m 27s (- 29m 1s) (10000 13%) 4.2664\n",
            "6m 35s (- 26m 21s) (15000 20%) 3.9971\n",
            "8m 50s (- 24m 19s) (20000 26%) 3.7742\n",
            "11m 0s (- 22m 1s) (25000 33%) 3.5385\n",
            "13m 10s (- 19m 45s) (30000 40%) 3.2970\n",
            "15m 19s (- 17m 31s) (35000 46%) 3.0348\n",
            "17m 28s (- 15m 17s) (40000 53%) 2.7957\n",
            "19m 38s (- 13m 5s) (45000 60%) 2.5653\n",
            "21m 46s (- 10m 53s) (50000 66%) 2.4001\n",
            "23m 56s (- 8m 42s) (55000 73%) 2.1450\n",
            "26m 5s (- 6m 31s) (60000 80%) 2.0174\n",
            "28m 15s (- 4m 20s) (65000 86%) 1.8899\n",
            "30m 26s (- 2m 10s) (70000 93%) 1.7675\n",
            "32m 37s (- 0m 0s) (75000 100%) 1.7408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx7Bkujpv34k",
        "outputId": "a4b5c84c-a276-49c3-a9fb-9468d2c4a7bd"
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> It s easy to get people to understand when you are trying to protect their rights .\n",
            "= 権利 を 守ろ う と する 方 が 、 周囲 の 理解 を 得られる\n",
            "< 権利 する 者 が 守ろ する する う と する う う と う <EOS>\n",
            "\n",
            "> We moved to our current house days after his birth .\n",
            "= 彼 が 生後 ４ ２ 日 目 に 今 の 家 に 引っ越す\n",
            "< 彼 が が ３ 、 彼 の 、 に か 思わず <EOS>\n",
            "\n",
            "> I overdo it .\n",
            "= 自分 が 無理 を する\n",
            "< 自分 が 書い て 書い た <EOS>\n",
            "\n",
            "> The number of people increases year by year .\n",
            "= 年 々人 の 数 が 増える\n",
            "< 年 が 数 が 増える <EOS>\n",
            "\n",
            "> He made an obscene remark .\n",
            "= 彼 が アダルト 的 発言 を し まし た\n",
            "< 彼 が 水玉 を 人 を し ます <EOS>\n",
            "\n",
            "> The following will be added to the level functions \n",
            "= レベル １ の 機能 に 下記 の 機能 を プラス する\n",
            "< 下記 者 が の 機能 の を を ます <EOS>\n",
            "\n",
            "> Someone said something like that .\n",
            "= そんな事 を 誰か が 口 に し た\n",
            "< その ほう が 、 そんな に し た <EOS>\n",
            "\n",
            "> X do does not come into the view at all .\n",
            "= X が 視界 に 全く 入り ませ ん\n",
            "< X が 全く 全く 全く は に を <EOS>\n",
            "\n",
            "> In that case you need to go through the re routing procedure .\n",
            "= その 場合 に は ルート 変更 の 手続き が 必要 に なり ます\n",
            "< その 場合 の 変更 に は 、 が が ます <EOS>\n",
            "\n",
            "> He shows children how hard he tries .\n",
            "= 彼 が 最大 限 の 努力 を し て いる 姿勢 を 子供 たち に 見せる\n",
            "< 彼 が お し 私 たち は の し し し し し し し ます し し <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNAUyqdWwHR-",
        "outputId": "70d0b2e1-485c-4615-953f-125b03e065b4"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShow(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "\n",
        "\n",
        "\n",
        "evaluateAndShow(\"I want to eat\")\n",
        "evaluateAndShow(\"I want to kill\")\n",
        "evaluateAndShow(\"I want to drink\")\n",
        "evaluateAndShow(\"I kill you\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = I want to eat\n",
            "output = 分解 組織 必ず多く みじん 蓄え 使う方 分かり 摂り お互い ほんとう 禁錮 必ず多く 禁錮 必ず多く 勧め がみんな 車窓 超え 瞑る 瞑る\n",
            "input = I want to kill\n",
            "output = 言わん 忙しく 過去 曲 被告 禁錮 必ず多く 横道 横道 禁錮 必ず多く 禁錮 必ず多く 勧め 過去 被告 今年 禁錮 必ず多く 横道\n",
            "input = I want to drink\n",
            "output = 店頭 全面 留学 禁錮 禁錮 図られ 図られ かける 短期 市販 実 新 全 禁錮 必ず多く 出力 禁錮 必ず多く 横道 横道\n",
            "input = I kill you\n",
            "output = 流れ パスタ 留学 潜ん 今年 筆者 禁錮 必ず多く 勧め 新 被告 禁錮 図られ 被告 短期 励まし 全 留学 禁錮 大正\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in0FGd5swRkL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}